{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej9242kSB5Dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "66ba0248-17e5-4f6b-bbe7-f16a08237c2f"
      },
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HAXOp52Fqy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eKPK-muF650",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
        "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
        "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1HJ5U5tPTCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f731b7a-2950-4766-bd0a-1a4e786da183"
      },
      "source": [
        "X.size()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1F-221ZQqAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e6c41e8-467b-4902-d9fb-7d06b8f66e9c"
      },
      "source": [
        "torch.max(X, 0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(values=tensor([1., 1.]), indices=tensor([2, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUMpU86HPeVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale units\n",
        "X_max, index = torch.max(X, 0) #dim=0 means across columns\n",
        "xPredicted_max, index = torch.max(xPredicted, 0)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FULRgVdOQyzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72f01aa0-4828-4121-93fe-157a1a7603a4"
      },
      "source": [
        "X_max"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cvn8SwcP9J8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.div(X, X_max)\n",
        "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
        "y = y / 100  # since max test score is 100"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqp8OJnSTLMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "14198efa-0186-4796-a848-f351a1f057ac"
      },
      "source": [
        "print(X)\n",
        "print(xPredicted_max)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6667, 1.0000],\n",
            "        [0.3333, 0.5556],\n",
            "        [1.0000, 0.6667]])\n",
            "tensor(8.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw1cb6WSbkwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the Neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKbM4v-wTPZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Neural_Network, self).__init__()\n",
        "        # parameters\n",
        "        # TODO: parameters can be parameterized instead of declaring them here\n",
        "        self.inputSize = 2\n",
        "        self.outputSize = 1\n",
        "        self.hiddenSize = 3\n",
        "        \n",
        "        # weights\n",
        "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 2 X 3 tensor\n",
        "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
        "        self.z2 = self.sigmoid(self.z) # activation function\n",
        "        self.z3 = torch.matmul(self.z2, self.W2)\n",
        "        o = self.sigmoid(self.z3) # final activation function\n",
        "        return o\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1 + torch.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        self.o_error = y - o # error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
        "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
        "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
        "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
        "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        # forward + backward pass for training\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n",
        "        \n",
        "    def saveWeights(self, model):\n",
        "        # we will use the PyTorch internal storage functions\n",
        "        torch.save(model, \"NN\")\n",
        "        # you can reload model with all the weights and so forth with:\n",
        "        # torch.load(\"NN\")\n",
        "        \n",
        "    def predict(self):\n",
        "        print (\"Predicted data based on trained weights: \")\n",
        "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
        "        y=self.forward(xPredicted)\n",
        "        \n",
        "        print(\" Output without scaling: \\n\"+str()) "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZK__HsUbnfk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36952415-6686-4aed-a00e-c23a3912ae74"
      },
      "source": [
        "NN = Neural_Network()\n",
        "for i in range(1000):  # trains the NN 1,000 times\n",
        "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "NN.saveWeights(NN)\n",
        "NN.predict()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0 Loss: 0.2444164901971817\n",
            "#1 Loss: 0.16411246359348297\n",
            "#2 Loss: 0.11083372682332993\n",
            "#3 Loss: 0.07788283377885818\n",
            "#4 Loss: 0.057453617453575134\n",
            "#5 Loss: 0.04431658983230591\n",
            "#6 Loss: 0.03548244759440422\n",
            "#7 Loss: 0.029285291209816933\n",
            "#8 Loss: 0.02477473020553589\n",
            "#9 Loss: 0.021387478336691856\n",
            "#10 Loss: 0.018775751814246178\n",
            "#11 Loss: 0.01671650819480419\n",
            "#12 Loss: 0.015061707235872746\n",
            "#13 Loss: 0.013710025697946548\n",
            "#14 Loss: 0.012590200640261173\n",
            "#15 Loss: 0.011650980450212955\n",
            "#16 Loss: 0.010854688473045826\n",
            "#17 Loss: 0.010173059068620205\n",
            "#18 Loss: 0.009584594517946243\n",
            "#19 Loss: 0.009072685614228249\n",
            "#20 Loss: 0.008624295704066753\n",
            "#21 Loss: 0.008229127153754234\n",
            "#22 Loss: 0.0078788623213768\n",
            "#23 Loss: 0.007566829677671194\n",
            "#24 Loss: 0.00728753162547946\n",
            "#25 Loss: 0.007036454975605011\n",
            "#26 Loss: 0.006809839513152838\n",
            "#27 Loss: 0.0066045657731592655\n",
            "#28 Loss: 0.006417965982109308\n",
            "#29 Loss: 0.006247814279049635\n",
            "#30 Loss: 0.006092197727411985\n",
            "#31 Loss: 0.005949476268142462\n",
            "#32 Loss: 0.0058182463981211185\n",
            "#33 Loss: 0.005697287153452635\n",
            "#34 Loss: 0.005585535895079374\n",
            "#35 Loss: 0.005482069682329893\n",
            "#36 Loss: 0.005386081058532\n",
            "#37 Loss: 0.005296859424561262\n",
            "#38 Loss: 0.005213767755776644\n",
            "#39 Loss: 0.005136263091117144\n",
            "#40 Loss: 0.005063841585069895\n",
            "#41 Loss: 0.004996058065444231\n",
            "#42 Loss: 0.0049325344152748585\n",
            "#43 Loss: 0.004872919525951147\n",
            "#44 Loss: 0.004816875793039799\n",
            "#45 Loss: 0.004764144774526358\n",
            "#46 Loss: 0.004714455455541611\n",
            "#47 Loss: 0.004667569417506456\n",
            "#48 Loss: 0.00462329899892211\n",
            "#49 Loss: 0.004581440705806017\n",
            "#50 Loss: 0.0045418147929012775\n",
            "#51 Loss: 0.004504271317273378\n",
            "#52 Loss: 0.004468669183552265\n",
            "#53 Loss: 0.004434869159013033\n",
            "#54 Loss: 0.004402755293995142\n",
            "#55 Loss: 0.004372207913547754\n",
            "#56 Loss: 0.004343139007687569\n",
            "#57 Loss: 0.004315445199608803\n",
            "#58 Loss: 0.004289036151021719\n",
            "#59 Loss: 0.004263847600668669\n",
            "#60 Loss: 0.004239787347614765\n",
            "#61 Loss: 0.004216811154037714\n",
            "#62 Loss: 0.0041948300786316395\n",
            "#63 Loss: 0.0041737970896065235\n",
            "#64 Loss: 0.004153660032898188\n",
            "#65 Loss: 0.004134359769523144\n",
            "#66 Loss: 0.00411586370319128\n",
            "#67 Loss: 0.004098116420209408\n",
            "#68 Loss: 0.00408107740804553\n",
            "#69 Loss: 0.004064714070409536\n",
            "#70 Loss: 0.004048986826092005\n",
            "#71 Loss: 0.004033863544464111\n",
            "#72 Loss: 0.0040193055756390095\n",
            "#73 Loss: 0.0040053012780845165\n",
            "#74 Loss: 0.0039918068796396255\n",
            "#75 Loss: 0.003978806082159281\n",
            "#76 Loss: 0.0039662704803049564\n",
            "#77 Loss: 0.003954170737415552\n",
            "#78 Loss: 0.00394250126555562\n",
            "#79 Loss: 0.003931234125047922\n",
            "#80 Loss: 0.003920344170182943\n",
            "#81 Loss: 0.003909821156412363\n",
            "#82 Loss: 0.003899645758792758\n",
            "#83 Loss: 0.003889799816533923\n",
            "#84 Loss: 0.003880278440192342\n",
            "#85 Loss: 0.0038710550870746374\n",
            "#86 Loss: 0.0038621174171566963\n",
            "#87 Loss: 0.0038534558843821287\n",
            "#88 Loss: 0.003845068858936429\n",
            "#89 Loss: 0.0038369239773601294\n",
            "#90 Loss: 0.0038290321826934814\n",
            "#91 Loss: 0.0038213597144931555\n",
            "#92 Loss: 0.0038139161188155413\n",
            "#93 Loss: 0.003806689055636525\n",
            "#94 Loss: 0.003799654310569167\n",
            "#95 Loss: 0.003792833536863327\n",
            "#96 Loss: 0.0037861859891563654\n",
            "#97 Loss: 0.0037797188851982355\n",
            "#98 Loss: 0.0037734368816018105\n",
            "#99 Loss: 0.0037673134356737137\n",
            "#100 Loss: 0.003761347383260727\n",
            "#101 Loss: 0.0037555384915322065\n",
            "#102 Loss: 0.0037498765159398317\n",
            "#103 Loss: 0.003744357032701373\n",
            "#104 Loss: 0.0037389665376394987\n",
            "#105 Loss: 0.003733719466254115\n",
            "#106 Loss: 0.003728589741513133\n",
            "#107 Loss: 0.0037235859781503677\n",
            "#108 Loss: 0.0037186990957707167\n",
            "#109 Loss: 0.0037139130290597677\n",
            "#110 Loss: 0.003709253156557679\n",
            "#111 Loss: 0.003704691305756569\n",
            "#112 Loss: 0.003700226778164506\n",
            "#113 Loss: 0.0036958660930395126\n",
            "#114 Loss: 0.0036915966775268316\n",
            "#115 Loss: 0.00368741643615067\n",
            "#116 Loss: 0.003683325834572315\n",
            "#117 Loss: 0.0036793167237192392\n",
            "#118 Loss: 0.003675392596051097\n",
            "#119 Loss: 0.0036715473979711533\n",
            "#120 Loss: 0.0036677720490843058\n",
            "#121 Loss: 0.003664070973172784\n",
            "#122 Loss: 0.003660450456663966\n",
            "#123 Loss: 0.0036568876821547747\n",
            "#124 Loss: 0.003653397783637047\n",
            "#125 Loss: 0.0036499679554253817\n",
            "#126 Loss: 0.003646598430350423\n",
            "#127 Loss: 0.0036432938650250435\n",
            "#128 Loss: 0.003640042617917061\n",
            "#129 Loss: 0.0036368537694215775\n",
            "#130 Loss: 0.0036337068304419518\n",
            "#131 Loss: 0.003630619263276458\n",
            "#132 Loss: 0.0036275843158364296\n",
            "#133 Loss: 0.0036245984956622124\n",
            "#134 Loss: 0.0036216590087860823\n",
            "#135 Loss: 0.003618760732933879\n",
            "#136 Loss: 0.003615907160565257\n",
            "#137 Loss: 0.0036131024826318026\n",
            "#138 Loss: 0.00361033552326262\n",
            "#139 Loss: 0.003607611171901226\n",
            "#140 Loss: 0.003604928031563759\n",
            "#141 Loss: 0.0036022772546857595\n",
            "#142 Loss: 0.003599666990339756\n",
            "#143 Loss: 0.003597090020775795\n",
            "#144 Loss: 0.003594554727897048\n",
            "#145 Loss: 0.0035920424852520227\n",
            "#146 Loss: 0.003589567495509982\n",
            "#147 Loss: 0.0035871260333806276\n",
            "#148 Loss: 0.0035847078543156385\n",
            "#149 Loss: 0.0035823306534439325\n",
            "#150 Loss: 0.0035799723118543625\n",
            "#151 Loss: 0.0035776409786194563\n",
            "#152 Loss: 0.0035753429401665926\n",
            "#153 Loss: 0.003573069116100669\n",
            "#154 Loss: 0.003570816246792674\n",
            "#155 Loss: 0.003568590385839343\n",
            "#156 Loss: 0.0035663985181599855\n",
            "#157 Loss: 0.0035642196889966726\n",
            "#158 Loss: 0.0035620590206235647\n",
            "#159 Loss: 0.0035599281545728445\n",
            "#160 Loss: 0.0035578198730945587\n",
            "#161 Loss: 0.003555725095793605\n",
            "#162 Loss: 0.0035536556970328093\n",
            "#163 Loss: 0.003551604226231575\n",
            "#164 Loss: 0.0035495732445269823\n",
            "#165 Loss: 0.0035475592594593763\n",
            "#166 Loss: 0.0035455599427223206\n",
            "#167 Loss: 0.0035435825120657682\n",
            "#168 Loss: 0.0035416146274656057\n",
            "#169 Loss: 0.0035396728198975325\n",
            "#170 Loss: 0.0035377424210309982\n",
            "#171 Loss: 0.003535830182954669\n",
            "#172 Loss: 0.0035339214373379946\n",
            "#173 Loss: 0.0035320392344146967\n",
            "#174 Loss: 0.003530169604346156\n",
            "#175 Loss: 0.003528305096551776\n",
            "#176 Loss: 0.003526463406160474\n",
            "#177 Loss: 0.003524632891640067\n",
            "#178 Loss: 0.0035228088963776827\n",
            "#179 Loss: 0.003520997939631343\n",
            "#180 Loss: 0.0035192023497074842\n",
            "#181 Loss: 0.0035174174699932337\n",
            "#182 Loss: 0.003515647491440177\n",
            "#183 Loss: 0.0035138812381774187\n",
            "#184 Loss: 0.00351213407702744\n",
            "#185 Loss: 0.0035103869158774614\n",
            "#186 Loss: 0.003508656518533826\n",
            "#187 Loss: 0.0035069293808192015\n",
            "#188 Loss: 0.00350521900691092\n",
            "#189 Loss: 0.0035035153850913048\n",
            "#190 Loss: 0.003501818748190999\n",
            "#191 Loss: 0.0035001318901777267\n",
            "#192 Loss: 0.0034984557423740625\n",
            "#193 Loss: 0.0034967840183526278\n",
            "#194 Loss: 0.0034951239358633757\n",
            "#195 Loss: 0.003493467578664422\n",
            "#196 Loss: 0.0034918177407234907\n",
            "#197 Loss: 0.0034901767503470182\n",
            "#198 Loss: 0.0034885413479059935\n",
            "#199 Loss: 0.0034869136288762093\n",
            "#200 Loss: 0.0034852984827011824\n",
            "#201 Loss: 0.0034836784470826387\n",
            "#202 Loss: 0.0034820723813027143\n",
            "#203 Loss: 0.003480469109490514\n",
            "#204 Loss: 0.0034788716584444046\n",
            "#205 Loss: 0.0034772802609950304\n",
            "#206 Loss: 0.003475695615634322\n",
            "#207 Loss: 0.003474117023870349\n",
            "#208 Loss: 0.0034725386649370193\n",
            "#209 Loss: 0.0034709686879068613\n",
            "#210 Loss: 0.003469402203336358\n",
            "#211 Loss: 0.003467838279902935\n",
            "#212 Loss: 0.003466288559138775\n",
            "#213 Loss: 0.003464736044406891\n",
            "#214 Loss: 0.003463188884779811\n",
            "#215 Loss: 0.003461642889305949\n",
            "#216 Loss: 0.0034600987564772367\n",
            "#217 Loss: 0.0034585685934871435\n",
            "#218 Loss: 0.0034570300485938787\n",
            "#219 Loss: 0.003455503610894084\n",
            "#220 Loss: 0.0034539822954684496\n",
            "#221 Loss: 0.0034524567890912294\n",
            "#222 Loss: 0.003450937569141388\n",
            "#223 Loss: 0.003449422074481845\n",
            "#224 Loss: 0.0034479096066206694\n",
            "#225 Loss: 0.0034463948104530573\n",
            "#226 Loss: 0.003444892121478915\n",
            "#227 Loss: 0.003443386172875762\n",
            "#228 Loss: 0.003441886743530631\n",
            "#229 Loss: 0.0034403821919113398\n",
            "#230 Loss: 0.0034388850908726454\n",
            "#231 Loss: 0.003437387989833951\n",
            "#232 Loss: 0.0034358978737145662\n",
            "#233 Loss: 0.0034344096202403307\n",
            "#234 Loss: 0.003432919504120946\n",
            "#235 Loss: 0.0034314340446144342\n",
            "#236 Loss: 0.00342994905076921\n",
            "#237 Loss: 0.003428470576182008\n",
            "#238 Loss: 0.003426986513659358\n",
            "#239 Loss: 0.003425508039072156\n",
            "#240 Loss: 0.0034240286331623793\n",
            "#241 Loss: 0.003422551555559039\n",
            "#242 Loss: 0.0034210802987217903\n",
            "#243 Loss: 0.0034196022897958755\n",
            "#244 Loss: 0.003418131498619914\n",
            "#245 Loss: 0.0034166581463068724\n",
            "#246 Loss: 0.003415185958147049\n",
            "#247 Loss: 0.0034137212205678225\n",
            "#248 Loss: 0.0034122541546821594\n",
            "#249 Loss: 0.0034107863903045654\n",
            "#250 Loss: 0.0034093186259269714\n",
            "#251 Loss: 0.003407855750992894\n",
            "#252 Loss: 0.0034063877537846565\n",
            "#253 Loss: 0.003404929069802165\n",
            "#254 Loss: 0.00340346060693264\n",
            "#255 Loss: 0.0034019991289824247\n",
            "#256 Loss: 0.0034005355555564165\n",
            "#257 Loss: 0.003399082226678729\n",
            "#258 Loss: 0.0033976174890995026\n",
            "#259 Loss: 0.003396156243979931\n",
            "#260 Loss: 0.0033947015181183815\n",
            "#261 Loss: 0.0033932386431843042\n",
            "#262 Loss: 0.0033917769324034452\n",
            "#263 Loss: 0.0033903184812515974\n",
            "#264 Loss: 0.003388861194252968\n",
            "#265 Loss: 0.003387399949133396\n",
            "#266 Loss: 0.0033859426621347666\n",
            "#267 Loss: 0.0033844851423054934\n",
            "#268 Loss: 0.003383025759831071\n",
            "#269 Loss: 0.003381568705663085\n",
            "#270 Loss: 0.003380111651495099\n",
            "#271 Loss: 0.00337864994071424\n",
            "#272 Loss: 0.003377192420884967\n",
            "#273 Loss: 0.0033757390920072794\n",
            "#274 Loss: 0.0033742792438715696\n",
            "#275 Loss: 0.003372821258381009\n",
            "#276 Loss: 0.0033713635057210922\n",
            "#277 Loss: 0.0033699043560773134\n",
            "#278 Loss: 0.0033684459049254656\n",
            "#279 Loss: 0.003366986522451043\n",
            "#280 Loss: 0.0033655252773314714\n",
            "#281 Loss: 0.003364064497873187\n",
            "#282 Loss: 0.00336260418407619\n",
            "#283 Loss: 0.0033611422404646873\n",
            "#284 Loss: 0.0033596784342080355\n",
            "#285 Loss: 0.0033582188189029694\n",
            "#286 Loss: 0.0033567536156624556\n",
            "#287 Loss: 0.00335528957657516\n",
            "#288 Loss: 0.003353829262778163\n",
            "#289 Loss: 0.0033523645251989365\n",
            "#290 Loss: 0.00335089978761971\n",
            "#291 Loss: 0.003349435282871127\n",
            "#292 Loss: 0.00334797240793705\n",
            "#293 Loss: 0.003346499288454652\n",
            "#294 Loss: 0.003345036879181862\n",
            "#295 Loss: 0.003343563759699464\n",
            "#296 Loss: 0.0033420973923057318\n",
            "#297 Loss: 0.0033406245056539774\n",
            "#298 Loss: 0.0033391574397683144\n",
            "#299 Loss: 0.003337683156132698\n",
            "#300 Loss: 0.0033362095709890127\n",
            "#301 Loss: 0.0033347371499985456\n",
            "#302 Loss: 0.0033332689199596643\n",
            "#303 Loss: 0.0033317937050014734\n",
            "#304 Loss: 0.0033303163945674896\n",
            "#305 Loss: 0.003328838385641575\n",
            "#306 Loss: 0.0033273613080382347\n",
            "#307 Loss: 0.0033258816692978144\n",
            "#308 Loss: 0.0033244004007428885\n",
            "#309 Loss: 0.003322925418615341\n",
            "#310 Loss: 0.003321445547044277\n",
            "#311 Loss: 0.0033199593890458345\n",
            "#312 Loss: 0.00331847439520061\n",
            "#313 Loss: 0.003316988004371524\n",
            "#314 Loss: 0.0033154997508972883\n",
            "#315 Loss: 0.0033140189480036497\n",
            "#316 Loss: 0.003312532091513276\n",
            "#317 Loss: 0.003311042906716466\n",
            "#318 Loss: 0.0033095479011535645\n",
            "#319 Loss: 0.0033080624416470528\n",
            "#320 Loss: 0.003306569764390588\n",
            "#321 Loss: 0.003305076388642192\n",
            "#322 Loss: 0.00330357882194221\n",
            "#323 Loss: 0.003302078926935792\n",
            "#324 Loss: 0.0033005804289132357\n",
            "#325 Loss: 0.0032990870531648397\n",
            "#326 Loss: 0.0032975857611745596\n",
            "#327 Loss: 0.0032960856333374977\n",
            "#328 Loss: 0.003294582711532712\n",
            "#329 Loss: 0.0032930849120020866\n",
            "#330 Loss: 0.003291577100753784\n",
            "#331 Loss: 0.003290073713287711\n",
            "#332 Loss: 0.0032885614782571793\n",
            "#333 Loss: 0.0032870538998395205\n",
            "#334 Loss: 0.003285546088591218\n",
            "#335 Loss: 0.003284035949036479\n",
            "#336 Loss: 0.0032825220841914415\n",
            "#337 Loss: 0.0032810065895318985\n",
            "#338 Loss: 0.0032794876024127007\n",
            "#339 Loss: 0.0032779762987047434\n",
            "#340 Loss: 0.0032764580100774765\n",
            "#341 Loss: 0.0032749390229582787\n",
            "#342 Loss: 0.0032734174747020006\n",
            "#343 Loss: 0.0032718926668167114\n",
            "#344 Loss: 0.003270376706495881\n",
            "#345 Loss: 0.0032688446808606386\n",
            "#346 Loss: 0.0032673177774995565\n",
            "#347 Loss: 0.00326579250395298\n",
            "#348 Loss: 0.0032642644364386797\n",
            "#349 Loss: 0.003262733109295368\n",
            "#350 Loss: 0.0032612003851681948\n",
            "#351 Loss: 0.003259666496887803\n",
            "#352 Loss: 0.0032581265550106764\n",
            "#353 Loss: 0.003256593830883503\n",
            "#354 Loss: 0.003255058778449893\n",
            "#355 Loss: 0.003253515576943755\n",
            "#356 Loss: 0.0032519735395908356\n",
            "#357 Loss: 0.0032504356931895018\n",
            "#358 Loss: 0.003248891094699502\n",
            "#359 Loss: 0.0032473457977175713\n",
            "#360 Loss: 0.0032457925844937563\n",
            "#361 Loss: 0.0032442451920360327\n",
            "#362 Loss: 0.0032426996622234583\n",
            "#363 Loss: 0.0032411396969109774\n",
            "#364 Loss: 0.003239596961066127\n",
            "#365 Loss: 0.0032380353659391403\n",
            "#366 Loss: 0.0032364826183766127\n",
            "#367 Loss: 0.0032349226530641317\n",
            "#368 Loss: 0.00323336198925972\n",
            "#369 Loss: 0.003231801325455308\n",
            "#370 Loss: 0.003230242757126689\n",
            "#371 Loss: 0.003228676738217473\n",
            "#372 Loss: 0.003227110719308257\n",
            "#373 Loss: 0.0032255437690764666\n",
            "#374 Loss: 0.003223974257707596\n",
            "#375 Loss: 0.0032223991584032774\n",
            "#376 Loss: 0.0032208303455263376\n",
            "#377 Loss: 0.0032192545477300882\n",
            "#378 Loss: 0.003217679215595126\n",
            "#379 Loss: 0.003216102719306946\n",
            "#380 Loss: 0.0032145229633897543\n",
            "#381 Loss: 0.0032129420433193445\n",
            "#382 Loss: 0.0032113606575876474\n",
            "#383 Loss: 0.0032097790390253067\n",
            "#384 Loss: 0.0032081922981888056\n",
            "#385 Loss: 0.003206602530553937\n",
            "#386 Loss: 0.0032050160225480795\n",
            "#387 Loss: 0.0032034243922680616\n",
            "#388 Loss: 0.003201839281246066\n",
            "#389 Loss: 0.003200242994353175\n",
            "#390 Loss: 0.0031986453104764223\n",
            "#391 Loss: 0.003197048557922244\n",
            "#392 Loss: 0.0031954487785696983\n",
            "#393 Loss: 0.0031938496977090836\n",
            "#394 Loss: 0.003192249918356538\n",
            "#395 Loss: 0.0031906499061733484\n",
            "#396 Loss: 0.0031890366226434708\n",
            "#397 Loss: 0.00318742822855711\n",
            "#398 Loss: 0.0031858207657933235\n",
            "#399 Loss: 0.0031842105090618134\n",
            "#400 Loss: 0.0031825981568545103\n",
            "#401 Loss: 0.0031809869688004255\n",
            "#402 Loss: 0.003179374150931835\n",
            "#403 Loss: 0.0031777562107890844\n",
            "#404 Loss: 0.0031761350110173225\n",
            "#405 Loss: 0.003174516372382641\n",
            "#406 Loss: 0.003172893775627017\n",
            "#407 Loss: 0.0031712709460407495\n",
            "#408 Loss: 0.0031696415971964598\n",
            "#409 Loss: 0.0031680145766586065\n",
            "#410 Loss: 0.003166388953104615\n",
            "#411 Loss: 0.003164755180478096\n",
            "#412 Loss: 0.0031631216406822205\n",
            "#413 Loss: 0.0031614915933459997\n",
            "#414 Loss: 0.0031598545610904694\n",
            "#415 Loss: 0.0031582138035446405\n",
            "#416 Loss: 0.0031565793324261904\n",
            "#417 Loss: 0.0031549346167594194\n",
            "#418 Loss: 0.0031532917637377977\n",
            "#419 Loss: 0.003151647048071027\n",
            "#420 Loss: 0.0031500032637268305\n",
            "#421 Loss: 0.0031483538914471865\n",
            "#422 Loss: 0.0031467017251998186\n",
            "#423 Loss: 0.003145052120089531\n",
            "#424 Loss: 0.0031434008851647377\n",
            "#425 Loss: 0.0031417440623044968\n",
            "#426 Loss: 0.00314008048735559\n",
            "#427 Loss: 0.0031384301837533712\n",
            "#428 Loss: 0.0031367705669254065\n",
            "#429 Loss: 0.003135107224807143\n",
            "#430 Loss: 0.0031334448140114546\n",
            "#431 Loss: 0.00313177308999002\n",
            "#432 Loss: 0.0031301071867346764\n",
            "#433 Loss: 0.003128443146124482\n",
            "#434 Loss: 0.0031267704907804728\n",
            "#435 Loss: 0.003125097369775176\n",
            "#436 Loss: 0.0031234233174473047\n",
            "#437 Loss: 0.0031217464711517096\n",
            "#438 Loss: 0.0031200703233480453\n",
            "#439 Loss: 0.003118384862318635\n",
            "#440 Loss: 0.0031167131382972\n",
            "#441 Loss: 0.0031150293070822954\n",
            "#442 Loss: 0.0031133443117141724\n",
            "#443 Loss: 0.0031116537284106016\n",
            "#444 Loss: 0.003109965706244111\n",
            "#445 Loss: 0.003108275355771184\n",
            "#446 Loss: 0.003106585470959544\n",
            "#447 Loss: 0.0031048886012285948\n",
            "#448 Loss: 0.0031031991820782423\n",
            "#449 Loss: 0.003101503476500511\n",
            "#450 Loss: 0.003099801717326045\n",
            "#451 Loss: 0.003098102053627372\n",
            "#452 Loss: 0.0030964005272835493\n",
            "#453 Loss: 0.003094692016020417\n",
            "#454 Loss: 0.0030929867643862963\n",
            "#455 Loss: 0.003091283142566681\n",
            "#456 Loss: 0.0030895716045051813\n",
            "#457 Loss: 0.0030878614634275436\n",
            "#458 Loss: 0.0030861487612128258\n",
            "#459 Loss: 0.003084440715610981\n",
            "#460 Loss: 0.0030827196314930916\n",
            "#461 Loss: 0.00308100413531065\n",
            "#462 Loss: 0.0030792795587331057\n",
            "#463 Loss: 0.003077564761042595\n",
            "#464 Loss: 0.003075835993513465\n",
            "#465 Loss: 0.00307411584071815\n",
            "#466 Loss: 0.003072390565648675\n",
            "#467 Loss: 0.0030706599354743958\n",
            "#468 Loss: 0.0030689288396388292\n",
            "#469 Loss: 0.003067196113988757\n",
            "#470 Loss: 0.00306546688079834\n",
            "#471 Loss: 0.0030637262389063835\n",
            "#472 Loss: 0.003061991883441806\n",
            "#473 Loss: 0.003060256829485297\n",
            "#474 Loss: 0.0030585182830691338\n",
            "#475 Loss: 0.0030567769426852465\n",
            "#476 Loss: 0.003055030247196555\n",
            "#477 Loss: 0.0030532863456755877\n",
            "#478 Loss: 0.00305153732188046\n",
            "#479 Loss: 0.0030497887637466192\n",
            "#480 Loss: 0.0030480388086289167\n",
            "#481 Loss: 0.0030462832655757666\n",
            "#482 Loss: 0.003044530749320984\n",
            "#483 Loss: 0.0030427814926952124\n",
            "#484 Loss: 0.0030410217586904764\n",
            "#485 Loss: 0.003039264352992177\n",
            "#486 Loss: 0.003037502057850361\n",
            "#487 Loss: 0.00303573627024889\n",
            "#488 Loss: 0.0030339770019054413\n",
            "#489 Loss: 0.003032212145626545\n",
            "#490 Loss: 0.003030442865565419\n",
            "#491 Loss: 0.003028671955689788\n",
            "#492 Loss: 0.003026902675628662\n",
            "#493 Loss: 0.003025130368769169\n",
            "#494 Loss: 0.003023352473974228\n",
            "#495 Loss: 0.0030215836595743895\n",
            "#496 Loss: 0.0030198025051504374\n",
            "#497 Loss: 0.0030180213507264853\n",
            "#498 Loss: 0.003016238333657384\n",
            "#499 Loss: 0.003014463698491454\n",
            "#500 Loss: 0.003012675791978836\n",
            "#501 Loss: 0.003010890679433942\n",
            "#502 Loss: 0.003009101143106818\n",
            "#503 Loss: 0.0030073169618844986\n",
            "#504 Loss: 0.0030055225361138582\n",
            "#505 Loss: 0.0030037295073270798\n",
            "#506 Loss: 0.0030019369442015886\n",
            "#507 Loss: 0.003000139957293868\n",
            "#508 Loss: 0.0029983408749103546\n",
            "#509 Loss: 0.0029965483117848635\n",
            "#510 Loss: 0.002994745736941695\n",
            "#511 Loss: 0.0029929440934211016\n",
            "#512 Loss: 0.0029911361634731293\n",
            "#513 Loss: 0.0029893305618315935\n",
            "#514 Loss: 0.0029875237960368395\n",
            "#515 Loss: 0.0029857156332582235\n",
            "#516 Loss: 0.0029839081689715385\n",
            "#517 Loss: 0.0029820932541042566\n",
            "#518 Loss: 0.0029802818316966295\n",
            "#519 Loss: 0.0029784676153212786\n",
            "#520 Loss: 0.0029766506049782038\n",
            "#521 Loss: 0.002974827541038394\n",
            "#522 Loss: 0.002973012626171112\n",
            "#523 Loss: 0.002971189096570015\n",
            "#524 Loss: 0.0029693657997995615\n",
            "#525 Loss: 0.002967539243400097\n",
            "#526 Loss: 0.0029657110571861267\n",
            "#527 Loss: 0.002963888691738248\n",
            "#528 Loss: 0.0029620565474033356\n",
            "#529 Loss: 0.00296022673137486\n",
            "#530 Loss: 0.0029583964496850967\n",
            "#531 Loss: 0.0029565610457211733\n",
            "#532 Loss: 0.0029547240119427443\n",
            "#533 Loss: 0.00295288790948689\n",
            "#534 Loss: 0.0029510485474020243\n",
            "#535 Loss: 0.002949208253994584\n",
            "#536 Loss: 0.0029473677277565002\n",
            "#537 Loss: 0.002945526735857129\n",
            "#538 Loss: 0.0029436785262078047\n",
            "#539 Loss: 0.002941832644864917\n",
            "#540 Loss: 0.002939986065030098\n",
            "#541 Loss: 0.0029381385538727045\n",
            "#542 Loss: 0.0029362887144088745\n",
            "#543 Loss: 0.002934432588517666\n",
            "#544 Loss: 0.002932579256594181\n",
            "#545 Loss: 0.00293072615750134\n",
            "#546 Loss: 0.002928868168964982\n",
            "#547 Loss: 0.0029270080849528313\n",
            "#548 Loss: 0.0029251507949084044\n",
            "#549 Loss: 0.002923292340710759\n",
            "#550 Loss: 0.0029214320238679647\n",
            "#551 Loss: 0.002919563790783286\n",
            "#552 Loss: 0.002917701378464699\n",
            "#553 Loss: 0.002915831282734871\n",
            "#554 Loss: 0.0029139623511582613\n",
            "#555 Loss: 0.0029120955150574446\n",
            "#556 Loss: 0.0029102216940373182\n",
            "#557 Loss: 0.002908349270001054\n",
            "#558 Loss: 0.002906475216150284\n",
            "#559 Loss: 0.002904601627960801\n",
            "#560 Loss: 0.002902725012972951\n",
            "#561 Loss: 0.0029008432757109404\n",
            "#562 Loss: 0.002898966195061803\n",
            "#563 Loss: 0.002897085389122367\n",
            "#564 Loss: 0.0028952036518603563\n",
            "#565 Loss: 0.0028933186549693346\n",
            "#566 Loss: 0.002891436219215393\n",
            "#567 Loss: 0.00288954540155828\n",
            "#568 Loss: 0.0028876580763608217\n",
            "#569 Loss: 0.002885769819840789\n",
            "#570 Loss: 0.0028838785365223885\n",
            "#571 Loss: 0.002881987253203988\n",
            "#572 Loss: 0.002880094340071082\n",
            "#573 Loss: 0.002878200961276889\n",
            "#574 Loss: 0.00287630339153111\n",
            "#575 Loss: 0.0028744060546159744\n",
            "#576 Loss: 0.002872508717700839\n",
            "#577 Loss: 0.0028706032317131758\n",
            "#578 Loss: 0.002868710085749626\n",
            "#579 Loss: 0.0028668080922216177\n",
            "#580 Loss: 0.0028648984152823687\n",
            "#581 Loss: 0.002862994559109211\n",
            "#582 Loss: 0.0028610865119844675\n",
            "#583 Loss: 0.0028591814916580915\n",
            "#584 Loss: 0.0028572718147188425\n",
            "#585 Loss: 0.0028553614392876625\n",
            "#586 Loss: 0.00285345152951777\n",
            "#587 Loss: 0.002851537661626935\n",
            "#588 Loss: 0.002849627286195755\n",
            "#589 Loss: 0.0028477117884904146\n",
            "#590 Loss: 0.00284579792059958\n",
            "#591 Loss: 0.0028438784647732973\n",
            "#592 Loss: 0.0028419606387615204\n",
            "#593 Loss: 0.0028400367591530085\n",
            "#594 Loss: 0.002838117303326726\n",
            "#595 Loss: 0.0028361936565488577\n",
            "#596 Loss: 0.002834271639585495\n",
            "#597 Loss: 0.002832346362993121\n",
            "#598 Loss: 0.0028304262086749077\n",
            "#599 Loss: 0.002828495344147086\n",
            "#600 Loss: 0.002826569601893425\n",
            "#601 Loss: 0.0028246387373656034\n",
            "#602 Loss: 0.002822711830958724\n",
            "#603 Loss: 0.0028207830619066954\n",
            "#604 Loss: 0.002818844048306346\n",
            "#605 Loss: 0.002816912718117237\n",
            "#606 Loss: 0.0028149783611297607\n",
            "#607 Loss: 0.0028130460996180773\n",
            "#608 Loss: 0.0028111033607274294\n",
            "#609 Loss: 0.0028091680724173784\n",
            "#610 Loss: 0.002807234413921833\n",
            "#611 Loss: 0.0028052926063537598\n",
            "#612 Loss: 0.002803350565955043\n",
            "#613 Loss: 0.0028014115523546934\n",
            "#614 Loss: 0.0027994681149721146\n",
            "#615 Loss: 0.0027975228149443865\n",
            "#616 Loss: 0.002795577049255371\n",
            "#617 Loss: 0.002793634543195367\n",
            "#618 Loss: 0.002791685052216053\n",
            "#619 Loss: 0.0027897395193576813\n",
            "#620 Loss: 0.0027877886313945055\n",
            "#621 Loss: 0.0027858372777700424\n",
            "#622 Loss: 0.002783892909064889\n",
            "#623 Loss: 0.0027819371316581964\n",
            "#624 Loss: 0.002779984148219228\n",
            "#625 Loss: 0.0027780348900705576\n",
            "#626 Loss: 0.0027760814409703016\n",
            "#627 Loss: 0.0027741193771362305\n",
            "#628 Loss: 0.002772168954834342\n",
            "#629 Loss: 0.00277020875364542\n",
            "#630 Loss: 0.0027682501822710037\n",
            "#631 Loss: 0.002766291843727231\n",
            "#632 Loss: 0.002764335600659251\n",
            "#633 Loss: 0.0027623705100268126\n",
            "#634 Loss: 0.002760410076007247\n",
            "#635 Loss: 0.002758447313681245\n",
            "#636 Loss: 0.0027564859483391047\n",
            "#637 Loss: 0.0027545213233679533\n",
            "#638 Loss: 0.002752554602921009\n",
            "#639 Loss: 0.0027505916077643633\n",
            "#640 Loss: 0.0027486225590109825\n",
            "#641 Loss: 0.0027466558385640383\n",
            "#642 Loss: 0.0027446935418993235\n",
            "#643 Loss: 0.0027427233289927244\n",
            "#644 Loss: 0.0027407540474087\n",
            "#645 Loss: 0.0027387843001633883\n",
            "#646 Loss: 0.0027368059381842613\n",
            "#647 Loss: 0.002734839217737317\n",
            "#648 Loss: 0.0027328680735081434\n",
            "#649 Loss: 0.0027308946009725332\n",
            "#650 Loss: 0.0027289194986224174\n",
            "#651 Loss: 0.0027269453275948763\n",
            "#652 Loss: 0.0027249676641076803\n",
            "#653 Loss: 0.0027229946572333574\n",
            "#654 Loss: 0.002721016528084874\n",
            "#655 Loss: 0.0027190400287508965\n",
            "#656 Loss: 0.0027170649264007807\n",
            "#657 Loss: 0.00271508633159101\n",
            "#658 Loss: 0.0027131037786602974\n",
            "#659 Loss: 0.0027111314702779055\n",
            "#660 Loss: 0.002709145424887538\n",
            "#661 Loss: 0.0027071693912148476\n",
            "#662 Loss: 0.002705182647332549\n",
            "#663 Loss: 0.002703206380829215\n",
            "#664 Loss: 0.0027012217324227095\n",
            "#665 Loss: 0.0026992373168468475\n",
            "#666 Loss: 0.0026972535997629166\n",
            "#667 Loss: 0.0026952698826789856\n",
            "#668 Loss: 0.0026932854671031237\n",
            "#669 Loss: 0.0026912970934063196\n",
            "#670 Loss: 0.0026893168687820435\n",
            "#671 Loss: 0.0026873277965933084\n",
            "#672 Loss: 0.002685336396098137\n",
            "#673 Loss: 0.0026833561714738607\n",
            "#674 Loss: 0.002681365003809333\n",
            "#675 Loss: 0.0026793808210641146\n",
            "#676 Loss: 0.0026773884892463684\n",
            "#677 Loss: 0.0026754008140414953\n",
            "#678 Loss: 0.002673414768651128\n",
            "#679 Loss: 0.0026714233681559563\n",
            "#680 Loss: 0.002669435925781727\n",
            "#681 Loss: 0.0026674410328269005\n",
            "#682 Loss: 0.0026654500979930162\n",
            "#683 Loss: 0.0026634579990059137\n",
            "#684 Loss: 0.002661469392478466\n",
            "#685 Loss: 0.002659476362168789\n",
            "#686 Loss: 0.0026574842631816864\n",
            "#687 Loss: 0.0026554910000413656\n",
            "#688 Loss: 0.002653498435392976\n",
            "#689 Loss: 0.002651509130373597\n",
            "#690 Loss: 0.002649518661201\n",
            "#691 Loss: 0.002647520275786519\n",
            "#692 Loss: 0.0026455267798155546\n",
            "#693 Loss: 0.0026435351464897394\n",
            "#694 Loss: 0.0026415411848574877\n",
            "#695 Loss: 0.0026395416352897882\n",
            "#696 Loss: 0.002637548604980111\n",
            "#697 Loss: 0.0026355574373155832\n",
            "#698 Loss: 0.002633558353409171\n",
            "#699 Loss: 0.00263156252913177\n",
            "#700 Loss: 0.0026295732241123915\n",
            "#701 Loss: 0.0026275680866092443\n",
            "#702 Loss: 0.002625575289130211\n",
            "#703 Loss: 0.0026235785335302353\n",
            "#704 Loss: 0.0026215820107609034\n",
            "#705 Loss: 0.0026195868849754333\n",
            "#706 Loss: 0.002617587335407734\n",
            "#707 Loss: 0.002615592675283551\n",
            "#708 Loss: 0.0026135987136512995\n",
            "#709 Loss: 0.0026115991640836\n",
            "#710 Loss: 0.002609602175652981\n",
            "#711 Loss: 0.0026076084468513727\n",
            "#712 Loss: 0.002605610527098179\n",
            "#713 Loss: 0.002603614702820778\n",
            "#714 Loss: 0.002601615386083722\n",
            "#715 Loss: 0.0025996179319918156\n",
            "#716 Loss: 0.0025976249016821384\n",
            "#717 Loss: 0.002595631405711174\n",
            "#718 Loss: 0.0025936258025467396\n",
            "#719 Loss: 0.002591631608083844\n",
            "#720 Loss: 0.002589638577774167\n",
            "#721 Loss: 0.002587641356512904\n",
            "#722 Loss: 0.0025856432039290667\n",
            "#723 Loss: 0.0025836455170065165\n",
            "#724 Loss: 0.00258165062405169\n",
            "#725 Loss: 0.0025796552654355764\n",
            "#726 Loss: 0.0025776538532227278\n",
            "#727 Loss: 0.0025756643153727055\n",
            "#728 Loss: 0.0025736652314662933\n",
            "#729 Loss: 0.002571669640019536\n",
            "#730 Loss: 0.0025696747470647097\n",
            "#731 Loss: 0.0025676775258034468\n",
            "#732 Loss: 0.002565686358138919\n",
            "#733 Loss: 0.0025636872742325068\n",
            "#734 Loss: 0.002561696106567979\n",
            "#735 Loss: 0.002559700747951865\n",
            "#736 Loss: 0.0025577035266906023\n",
            "#737 Loss: 0.0025557067710906267\n",
            "#738 Loss: 0.0025537132751196623\n",
            "#739 Loss: 0.002551720477640629\n",
            "#740 Loss: 0.0025497267488390207\n",
            "#741 Loss: 0.002547736279666424\n",
            "#742 Loss: 0.002545741153880954\n",
            "#743 Loss: 0.002543748589232564\n",
            "#744 Loss: 0.002541757421568036\n",
            "#745 Loss: 0.002539762295782566\n",
            "#746 Loss: 0.0025377734564244747\n",
            "#747 Loss: 0.0025357864797115326\n",
            "#748 Loss: 0.0025337913539260626\n",
            "#749 Loss: 0.002531804144382477\n",
            "#750 Loss: 0.002529809018597007\n",
            "#751 Loss: 0.002527824370190501\n",
            "#752 Loss: 0.002525829942896962\n",
            "#753 Loss: 0.0025238438975065947\n",
            "#754 Loss: 0.002521853893995285\n",
            "#755 Loss: 0.0025198657531291246\n",
            "#756 Loss: 0.0025178808718919754\n",
            "#757 Loss: 0.0025158869102597237\n",
            "#758 Loss: 0.0025139031931757927\n",
            "#759 Loss: 0.0025119187775999308\n",
            "#760 Loss: 0.002509932266548276\n",
            "#761 Loss: 0.002507947152480483\n",
            "#762 Loss: 0.0025059629697352648\n",
            "#763 Loss: 0.002503977855667472\n",
            "#764 Loss: 0.0025019946042448282\n",
            "#765 Loss: 0.002500012284144759\n",
            "#766 Loss: 0.0024980318266898394\n",
            "#767 Loss: 0.0024960532318800688\n",
            "#768 Loss: 0.0024940648581832647\n",
            "#769 Loss: 0.002492083702236414\n",
            "#770 Loss: 0.002490103943273425\n",
            "#771 Loss: 0.0024881286080926657\n",
            "#772 Loss: 0.0024861476849764585\n",
            "#773 Loss: 0.0024841693229973316\n",
            "#774 Loss: 0.0024821918923407793\n",
            "#775 Loss: 0.002480214461684227\n",
            "#776 Loss: 0.002478240989148617\n",
            "#777 Loss: 0.0024762607645243406\n",
            "#778 Loss: 0.002474286826327443\n",
            "#779 Loss: 0.0024723121896386147\n",
            "#780 Loss: 0.002470343839377165\n",
            "#781 Loss: 0.0024683671072125435\n",
            "#782 Loss: 0.002466393867507577\n",
            "#783 Loss: 0.0024644238874316216\n",
            "#784 Loss: 0.0024624562356621027\n",
            "#785 Loss: 0.0024604874197393656\n",
            "#786 Loss: 0.002458513481542468\n",
            "#787 Loss: 0.002456547459587455\n",
            "#788 Loss: 0.0024545786436647177\n",
            "#789 Loss: 0.002452613553032279\n",
            "#790 Loss: 0.002450647298246622\n",
            "#791 Loss: 0.0024486847687512636\n",
            "#792 Loss: 0.0024467166513204575\n",
            "#793 Loss: 0.002444753423333168\n",
            "#794 Loss: 0.002442793920636177\n",
            "#795 Loss: 0.002440830459818244\n",
            "#796 Loss: 0.002438873052597046\n",
            "#797 Loss: 0.002436912152916193\n",
            "#798 Loss: 0.0024349556770175695\n",
            "#799 Loss: 0.002432994544506073\n",
            "#800 Loss: 0.0024310406297445297\n",
            "#801 Loss: 0.0024290813598781824\n",
            "#802 Loss: 0.002427127445116639\n",
            "#803 Loss: 0.0024251751601696014\n",
            "#804 Loss: 0.0024232259020209312\n",
            "#805 Loss: 0.0024212661664932966\n",
            "#806 Loss: 0.0024193141143769026\n",
            "#807 Loss: 0.0024173662532120943\n",
            "#808 Loss: 0.002415416529402137\n",
            "#809 Loss: 0.0024134686682373285\n",
            "#810 Loss: 0.0024115184787660837\n",
            "#811 Loss: 0.002409575739875436\n",
            "#812 Loss: 0.002407628810033202\n",
            "#813 Loss: 0.0024056860711425543\n",
            "#814 Loss: 0.002403742866590619\n",
            "#815 Loss: 0.0024018019903451204\n",
            "#816 Loss: 0.0023998646065592766\n",
            "#817 Loss: 0.0023979232646524906\n",
            "#818 Loss: 0.002395983785390854\n",
            "#819 Loss: 0.0023940447717905045\n",
            "#820 Loss: 0.002392109017819166\n",
            "#821 Loss: 0.0023901748936623335\n",
            "#822 Loss: 0.002388241933658719\n",
            "#823 Loss: 0.0023863131646066904\n",
            "#824 Loss: 0.0023843753151595592\n",
            "#825 Loss: 0.0023824500385671854\n",
            "#826 Loss: 0.002380521036684513\n",
            "#827 Loss: 0.002378588542342186\n",
            "#828 Loss: 0.002376665361225605\n",
            "#829 Loss: 0.0023747372906655073\n",
            "#830 Loss: 0.002372812945395708\n",
            "#831 Loss: 0.0023708909284323454\n",
            "#832 Loss: 0.002368971472606063\n",
            "#833 Loss: 0.0023670485243201256\n",
            "#834 Loss: 0.0023651316296309233\n",
            "#835 Loss: 0.0023632098454982042\n",
            "#836 Loss: 0.0023612906225025654\n",
            "#837 Loss: 0.0023593821097165346\n",
            "#838 Loss: 0.002357465447857976\n",
            "#839 Loss: 0.0023555532097816467\n",
            "#840 Loss: 0.0023536430671811104\n",
            "#841 Loss: 0.0023517326917499304\n",
            "#842 Loss: 0.002349820686504245\n",
            "#843 Loss: 0.0023479170631617308\n",
            "#844 Loss: 0.0023460097145289183\n",
            "#845 Loss: 0.002344102831557393\n",
            "#846 Loss: 0.0023422015365213156\n",
            "#847 Loss: 0.002340296981856227\n",
            "#848 Loss: 0.002338398015126586\n",
            "#849 Loss: 0.0023364981170743704\n",
            "#850 Loss: 0.002334603341296315\n",
            "#851 Loss: 0.0023327027447521687\n",
            "#852 Loss: 0.0023308084346354008\n",
            "#853 Loss: 0.002328920643776655\n",
            "#854 Loss: 0.0023270242381840944\n",
            "#855 Loss: 0.0023251334205269814\n",
            "#856 Loss: 0.0023232472594827414\n",
            "#857 Loss: 0.0023213548120111227\n",
            "#858 Loss: 0.00231947167776525\n",
            "#859 Loss: 0.0023175866808742285\n",
            "#860 Loss: 0.0023157040122896433\n",
            "#861 Loss: 0.0023138185497373343\n",
            "#862 Loss: 0.002311940072104335\n",
            "#863 Loss: 0.0023100615944713354\n",
            "#864 Loss: 0.0023081835824996233\n",
            "#865 Loss: 0.002306306269019842\n",
            "#866 Loss: 0.002304432447999716\n",
            "#867 Loss: 0.002302560955286026\n",
            "#868 Loss: 0.002300691558048129\n",
            "#869 Loss: 0.002298824256286025\n",
            "#870 Loss: 0.0022969578858464956\n",
            "#871 Loss: 0.0022950901184231043\n",
            "#872 Loss: 0.002293224446475506\n",
            "#873 Loss: 0.002291364362463355\n",
            "#874 Loss: 0.002289500320330262\n",
            "#875 Loss: 0.0022876395378261805\n",
            "#876 Loss: 0.002285779919475317\n",
            "#877 Loss: 0.0022839251905679703\n",
            "#878 Loss: 0.002282068133354187\n",
            "#879 Loss: 0.0022802159655839205\n",
            "#880 Loss: 0.0022783679887652397\n",
            "#881 Loss: 0.002276515355333686\n",
            "#882 Loss: 0.002274664817377925\n",
            "#883 Loss: 0.002272819634526968\n",
            "#884 Loss: 0.0022709777113050222\n",
            "#885 Loss: 0.0022691318299621344\n",
            "#886 Loss: 0.002267291536554694\n",
            "#887 Loss: 0.002265448682010174\n",
            "#888 Loss: 0.0022636118810623884\n",
            "#889 Loss: 0.002261776477098465\n",
            "#890 Loss: 0.0022599410731345415\n",
            "#891 Loss: 0.002258104970678687\n",
            "#892 Loss: 0.0022562758531421423\n",
            "#893 Loss: 0.0022544467356055975\n",
            "#894 Loss: 0.0022526204120367765\n",
            "#895 Loss: 0.0022507943212985992\n",
            "#896 Loss: 0.0022489712573587894\n",
            "#897 Loss: 0.0022471484262496233\n",
            "#898 Loss: 0.0022453300189226866\n",
            "#899 Loss: 0.0022435090504586697\n",
            "#900 Loss: 0.0022416922729462385\n",
            "#901 Loss: 0.002239874331280589\n",
            "#902 Loss: 0.0022380624432116747\n",
            "#903 Loss: 0.002236254047602415\n",
            "#904 Loss: 0.002234440064057708\n",
            "#905 Loss: 0.0022326370235532522\n",
            "#906 Loss: 0.0022308307234197855\n",
            "#907 Loss: 0.0022290244232863188\n",
            "#908 Loss: 0.0022272190544754267\n",
            "#909 Loss: 0.0022254197392612696\n",
            "#910 Loss: 0.0022236208897083998\n",
            "#911 Loss: 0.002221823437139392\n",
            "#912 Loss: 0.002220028778538108\n",
            "#913 Loss: 0.002218235982581973\n",
            "#914 Loss: 0.0022164422553032637\n",
            "#915 Loss: 0.002214654115960002\n",
            "#916 Loss: 0.002212866907939315\n",
            "#917 Loss: 0.002211084356531501\n",
            "#918 Loss: 0.0022092971485108137\n",
            "#919 Loss: 0.00220751971937716\n",
            "#920 Loss: 0.0022057334426790476\n",
            "#921 Loss: 0.002203963929787278\n",
            "#922 Loss: 0.002202181378379464\n",
            "#923 Loss: 0.002200407674536109\n",
            "#924 Loss: 0.0021986353676766157\n",
            "#925 Loss: 0.0021968649234622717\n",
            "#926 Loss: 0.0021950965747237206\n",
            "#927 Loss: 0.002193332416936755\n",
            "#928 Loss: 0.002191568724811077\n",
            "#929 Loss: 0.002189806429669261\n",
            "#930 Loss: 0.002188043901696801\n",
            "#931 Loss: 0.0021862878929823637\n",
            "#932 Loss: 0.0021845309529453516\n",
            "#933 Loss: 0.0021827733144164085\n",
            "#934 Loss: 0.0021810224279761314\n",
            "#935 Loss: 0.002179271774366498\n",
            "#936 Loss: 0.0021775236818939447\n",
            "#937 Loss: 0.0021757769864052534\n",
            "#938 Loss: 0.002174033084884286\n",
            "#939 Loss: 0.002172289416193962\n",
            "#940 Loss: 0.0021705497056245804\n",
            "#941 Loss: 0.002168806968256831\n",
            "#942 Loss: 0.0021670751739293337\n",
            "#943 Loss: 0.002165340119972825\n",
            "#944 Loss: 0.0021636062301695347\n",
            "#945 Loss: 0.0021618774626404047\n",
            "#946 Loss: 0.002160145901143551\n",
            "#947 Loss: 0.0021584222558885813\n",
            "#948 Loss: 0.0021566960494965315\n",
            "#949 Loss: 0.002154977060854435\n",
            "#950 Loss: 0.0021532566752284765\n",
            "#951 Loss: 0.0021515407133847475\n",
            "#952 Loss: 0.0021498233545571566\n",
            "#953 Loss: 0.0021481087896972895\n",
            "#954 Loss: 0.0021463932935148478\n",
            "#955 Loss: 0.0021446822211146355\n",
            "#956 Loss: 0.0021429783664643764\n",
            "#957 Loss: 0.002141273580491543\n",
            "#958 Loss: 0.0021395704243332148\n",
            "#959 Loss: 0.0021378665696829557\n",
            "#960 Loss: 0.002136167837306857\n",
            "#961 Loss: 0.002134475624188781\n",
            "#962 Loss: 0.0021327773574739695\n",
            "#963 Loss: 0.002131082583218813\n",
            "#964 Loss: 0.002129392698407173\n",
            "#965 Loss: 0.0021277042105793953\n",
            "#966 Loss: 0.002126014558598399\n",
            "#967 Loss: 0.0021243335213512182\n",
            "#968 Loss: 0.0021226508542895317\n",
            "#969 Loss: 0.0021209712140262127\n",
            "#970 Loss: 0.0021192890126258135\n",
            "#971 Loss: 0.002117614494636655\n",
            "#972 Loss: 0.002115941606462002\n",
            "#973 Loss: 0.002114269882440567\n",
            "#974 Loss: 0.002112603047862649\n",
            "#975 Loss: 0.002110934117808938\n",
            "#976 Loss: 0.002109271241351962\n",
            "#977 Loss: 0.002107606502249837\n",
            "#978 Loss: 0.0021059431601315737\n",
            "#979 Loss: 0.002104283543303609\n",
            "#980 Loss: 0.0021026283502578735\n",
            "#981 Loss: 0.0021009701304137707\n",
            "#982 Loss: 0.002099319128319621\n",
            "#983 Loss: 0.0020976730156689882\n",
            "#984 Loss: 0.002096023177728057\n",
            "#985 Loss: 0.0020943754352629185\n",
            "#986 Loss: 0.0020927335135638714\n",
            "#987 Loss: 0.0020910922903567553\n",
            "#988 Loss: 0.0020894508343189955\n",
            "#989 Loss: 0.002087814500555396\n",
            "#990 Loss: 0.0020861795637756586\n",
            "#991 Loss: 0.002084548817947507\n",
            "#992 Loss: 0.0020829204004257917\n",
            "#993 Loss: 0.002081291051581502\n",
            "#994 Loss: 0.002079661935567856\n",
            "#995 Loss: 0.002078039338812232\n",
            "#996 Loss: 0.002076416742056608\n",
            "#997 Loss: 0.0020747974049299955\n",
            "#998 Loss: 0.0020731762051582336\n",
            "#999 Loss: 0.0020715652499347925\n",
            "Predicted data based on trained weights: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "tensor([0.9551])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}